### Hi there 👋 This is Minshen Zhang
[![Github](https://img.shields.io/badge/-Github-000?style=flat&logo=Github&logoColor=white)](https://github.com/alexzms)
[![Linkedin](https://img.shields.io/badge/-LinkedIn-blue?style=flat&logo=Linkedin&logoColor=white)](https://www.linkedin.com/in/minshen-zhang-416a0b291/)
[![Personal Website](https://img.shields.io/badge/-Website-47CCCC?style=flat&logo=Google-Chrome&logoColor=white)](https://alexzms.github.io)
[![Arximia](https://img.shields.io/badge/-Arximia-FF4088?style=flat&logo=Vercel&logoColor=white)](https://arximia.com)
[![Gmail](https://img.shields.io/badge/-Gmail-c14438?style=flat&logo=Gmail&logoColor=white)](mailto:alexzhangminshen@gmail.com)

- 🎓 MS in Computer Science at UC San Diego (2025-2027)
  - BS in Computer Science at ShanghaiTech University (2021-2025)
  - GPA: 3.69/4.0 (Overall), 3.8+/4.0 (Major)
  - Exchange student at UC Berkeley Extension (GLOBE Program) with 4.0/4.0 GPA
    
- 🔬 Research Experience
  - Shanghai Alibaba Ant Group NLP Lab (Jul. 2025-Present): Researching Multi-Head FFN as a powerful and efficient alternative to FFNs in Transformers.
  - ShanghaiTech Kewei Tu's Lab (Feb. 2025-Present): Using sparse autoencoders to probe entity-specific knowledge within LLMs.
  - Shanghai Qizhi Institute (Jun. 2024-Jan. 2025): Developed an analytical solution for real-time inverse-refraction problems.
    
- 📝 Publications
  - **Flash Multi-Head Feed-Forward Networks** (Under Review)
    - Minshen Zhang*, Xiang Hu*, Jianguo Li, Wei Wu, Kewei Tu
    - *FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x.*
      
- 🔭 I'm currently working on
  - Flash Multi-Head FFN and other novel LLM structures
  - Interesting problems in Machine Learning systems.
  - Retrieval based Long-context end-to-end language modeling systems.
    
- 🌱 Skills & Technologies
  - **MLsys**: PyTorch, Triton, ThunderKittens
  - **CUDA C++**: Implemented custom Flash Algorithm on Hopper with warp specialization and WMMA.
  - **Libraries**: Hugging Face Transformers (Authored a merged PR)
  - **Languages**: Python, C/C++, C
  
- 🏆 Honors
  - Outstanding Graduate of ShanghaiTech University (2024-2025)
  - Outstanding Student of ShanghaiTech University (2021-2022, 2023-2024)
- 👨‍🏫 Teaching: Teaching Assistant for Computer Programming (CS100) at ShanghaiTech University (Spring 2024)

<!-- <img width="50%" align="right" src="https://github-readme-stats.vercel.app/api?username=alexzms&show_icons=true&hide_border=true&icon_color=586069&title_color=a0a9af">
<div align="center"> <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=alexzms&hide_title=true&hide_border=true&layout=compact&langs_count=6&text_color=000&icon_color=fff&bg_color=0,52fa5a,4dfcff,c64dff&theme=graywhite" /> </div> -->
